<!DOCTYPE html>
<meta charset="UTF-8"> 

<html>
	<head>
		<title>Michael's Lab</title>
		<link rel = stylesheet href = "main.css" type = "text/css">
		<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	</head>

	<body>

		<h1>Michael's Lab</h1>
		<script src="csi.js"></script>
		<script src="navbar.js"></script>
		<div data-include="navbar.html"></div>

		<h2>Deming Regression</h2>

		<p>
		The Deming regression is a linear regression (or, "line of best fit"), that minimizes the total Euclidean squared distance of every point in a set from that line.  The key difference is that the distance of every point to the line has both a x and y component, not just a vertical distance.
		</p>

		<p>
		This regression is classically a minimization problem of two variables: slope, and y-intersect.  Here, I use the Hessian normal form of the line, which naturally extends to planes.  It is essentially a polar way of describing lines: in terms of angle from the origin, and distance.
		</p>

		<canvas id="canvas" width="500" height="500"></canvas>

		<p>
		I'm using a simple optimization algorithm based on the numerical partial derivatives of the two variables with respect to the score.  A property of the orthogonal least-squares linear regression method is that there are no local minimums.  This is probably related to the fact that there is a closed form solution for the line.
		</p>

		<p>
		Of note, is that the line does indeed pass through the simple average of the points.  As well, the behavior of the line as it finds the minimum score can be compared to physical spring systems.
		</p>

		<h3>Drawbacks</h3>
		<p>
		The Deming regression, or orthogonal regression, weighs outliers heavily.  Although it has deep statistical advantages, the following case definitely seems like an error in the context of fitting arbitrary points.  A human would definitely not say the line of best fit is this:
		</p>

		<img src="demingfiterror.png" style="display:block;margin: 0 auto;">

		<p>
		You can build such a case with the above tool yourself.  It will disregard very obvious linear relationships if there is an outlier of moderate distance.  Indeed, it will always disregard some finite relationship of points - no matter how strong - if there is an arbitrarily far away outlier.  This is also the case with the ' least absolute error regression' method, which simply adds up the distance of every point from the line. (And has local minima).
		</p>

		<p>
		One hypothetical cost function which would not have this property is perhaps one that maximizes the sum of 1/(dist)^2 of every point.  This resembles cetain other equations from physics.
		</p>

		<p>
		Building a list of local minima may allow one to find multiple, distinct, linear relationships in the same set of points with efficient complexity.  This is leaving the domain of linear regression, and entering into topics such as data mining and pattern recognition.
		</p>

		<div id = "footer"></div>

		<script type="text/javascript" src="demingregression.js"></script>
	</body>
</html>